{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Old9f6GURNrc"
   },
   "source": [
    "**Bidirectional LSTM in Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QmGPGx20OZ0z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5YndUJJ1RTyF"
   },
   "outputs": [],
   "source": [
    "#Input Parameters\n",
    "\n",
    "#input_size = The number of expected features in the input. Feature dimension\n",
    "\n",
    "#hidden_size = Number of units in the hidden state. The number of features in the hidden state h\n",
    "\n",
    "#num_layers = Number of vertical stacks of hidden layers\n",
    "\n",
    "#batch_first = True, it means input shape of the data to the LSTM is (batch_size, seq_len, features)\n",
    "\n",
    "#bidirectional = True, it menas that LSTM is bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pBIkOO_KRocJ"
   },
   "outputs": [],
   "source": [
    "#nn.LSTM( input_size, hidden_size, num_layers, batch_first = True, bidirectional = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yytLK634R2R4"
   },
   "source": [
    "When batch_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6MTNcjjR5RV",
    "outputId": "981d2632-d272-47f4-fef3-3ed0ab2656b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(1, 16, batch_first=True, bidirectional=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set parameters\n",
    "input_size  =  1\n",
    "hidden_size = 16\n",
    "num_layers  =  1\n",
    "\n",
    "#Create Model\n",
    "Bi_lstmModel = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
    "Bi_lstmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4EWEmn85SIfe"
   },
   "outputs": [],
   "source": [
    "#Creating data\n",
    "seqlength = 5 # sequence length or timesteps\n",
    "batchsize = 4 # Batch size or Number of samples\n",
    "\n",
    "# create some random data\n",
    "X = torch.rand(batchsize, seqlength, input_size) # Because batch_first is true, therefore (N,T,D)\n",
    "                                                 # N = Batch size or Number of samples\n",
    "                                                 # T = sequence length or timesteps\n",
    "                                                 # D = Feature Dimension or Inputsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOGRi38zSQZt",
    "outputId": "c5ba3a6c-9353-4973-9fe3-f61a014bdec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [4, 5, 1]\n",
      "Hidden shape: [2, 4, 16]\n",
      "Cell memory shape: [2, 4, 16]\n",
      "Output shape: [4, 5, 32]\n"
     ]
    }
   ],
   "source": [
    "#Get output from model\n",
    "y,(h,c) = Bi_lstmModel(X) #hidden_inputs)\n",
    "\n",
    "print(f'Input shape: {list(X.shape)}')            # Batchsize x seqlen x feature dim  OR N x T x D\n",
    "print(f'Hidden shape: {list(h.shape)}')           # 2 * numlayers x Batchsize x hiddensize\n",
    "print(f'Cell memory shape: {list(c.shape)}')      # 2 * numlayers x Batchsize x hiddensize\n",
    "print(f'Output shape: {list(y.shape)}')           # batchsize x seqlen x 2 * hiddensize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt1zTZwISXEi"
   },
   "source": [
    "When batch_first = False (defalut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAfSf2VRSZFu",
    "outputId": "da2c11c5-3e0e-4865-96b9-8f0e8968cccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(1, 16, bidirectional=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bi_lstmModel2 = nn.LSTM(input_size, hidden_size, num_layers, bidirectional = True)\n",
    "Bi_lstmModel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "71ujh6k6SfIr"
   },
   "outputs": [],
   "source": [
    "X = torch.rand(seqlength, batchsize, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzzXGLvcShSb",
    "outputId": "61650fe1-61e5-4256-c769-0f6926596489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [5, 4, 1]\n",
      "Hidden shape: [2, 4, 16]\n",
      "Cell memory shape: [2, 4, 16]\n",
      "Output shape: [5, 4, 32]\n"
     ]
    }
   ],
   "source": [
    "y,(h,c) = Bi_lstmModel2(X) #hidden_inputs)\n",
    "\n",
    "print(f'Input shape: {list(X.shape)}')            # seqlen x Batchsize x feature dim  OR N x T x D\n",
    "print(f'Hidden shape: {list(h.shape)}')           # 2 * numlayers x Batchsize x hiddensize\n",
    "print(f'Cell memory shape: {list(c.shape)}')      # 2 * numlayers x Batchsize x hiddensize\n",
    "print(f'Output shape: {list(y.shape)}')           # seqlen x Batchsize x 2 * hiddensize"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
