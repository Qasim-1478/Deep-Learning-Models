{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oiu_4VcHVUHJ"
   },
   "outputs": [],
   "source": [
    "#Basic Shapes of the Transformer encoder for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "am3JjikEVZnM"
   },
   "outputs": [],
   "source": [
    "# Input Layer -> Post Encoding -> Encoder Layer -> Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YlA1JREiVolk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIU0yjz-VsUn",
    "outputId": "d2c7cb78-9ff2-437e-c8c5-05f3a2d7fb20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instance of encoder layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model = 256, nhead = 8) #d_model is the input size\n",
    "encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lnSi58iwVzTD"
   },
   "outputs": [],
   "source": [
    "#Create Data\n",
    "x = torch.randn(32, 256) # Batchsize x seqlen. seqlen must be equal to the embedded dimension of the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKpgPRifV5Qd",
    "outputId": "c3f76104-cba2-4c1f-8bc8-0124d9a29c57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1345,  0.1916, -0.3283,  ..., -0.9822,  0.8215, -1.0673],\n",
       "        [-0.1567,  0.4350, -1.7306,  ...,  1.8023,  0.3061, -1.3368],\n",
       "        [ 0.7932,  0.1927,  0.2961,  ..., -2.0422,  0.2612, -0.2270],\n",
       "        ...,\n",
       "        [ 0.6337,  1.1824,  1.2053,  ...,  0.2002, -0.1593,  0.1517],\n",
       "        [-0.3707,  1.7830, -1.8302,  ...,  1.3932, -0.7683,  0.0058],\n",
       "        [ 0.0623,  1.3671, -0.2881,  ...,  0.3810, -1.1351, -0.1194]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get output from encoder layer\n",
    "out_encoder_layer = encoder_layer(x)\n",
    "out_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Np6mEEPGWHjA",
    "outputId": "83c5aee7-7413-491f-f3dc-708f6dff125d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encoder_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nR64jRqgWJrR",
    "outputId": "2d589e19-ba9a-40bb-e5d0-4b41dfdf52c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data 2\n",
    "\n",
    "x2 = torch.rand(32, 1, 185)\n",
    "x2 = nn.Linear(185, 256)(x2)\n",
    "out_encoder_layer2 = encoder_layer(x2)\n",
    "out_encoder_layer2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dsh6fug1WYdK",
    "outputId": "695d491d-18b2-4f74-af03-15ba392d4fee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instancing the encoder\n",
    "num_layers = 6\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GJwu-t0WiXS",
    "outputId": "397357d9-b62f-4a1f-ca56-e35d5c9dbeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encoder = encoder(x)\n",
    "out_encoder.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
